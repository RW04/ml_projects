{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Classification, Decision Trees and k Nearest Neighbors</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Contents\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Decision Tree](#2.-Decision-Tree)\n",
    "3. [Nearest Neighbors Method](#3.-Nearest-Neighbors-Method)\n",
    "4. [Choosing Model Parameters and Cross-Validation](#4.-Choosing-Model-Parameters-and-Cross-Validation)\n",
    "5. [Application Examples and Complex Cases](#5.-Application-Examples-and-Complex-Cases)\n",
    "6. [Pros and Cons of Decision Trees and the Nearest Neighbors Method](#6.-Pros-and-Cons-of-Decision-Trees-and-the-Nearest-Neighbors-Method)\n",
    "7. [Useful Resources](#7.-Useful-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "- #### What is Machine Learning?\n",
    "     \n",
    "  T. Mitchell's book Machine Learning (1997) gives a classic, general definition of machine learning as follows:\n",
    "\n",
    "    > A computer program is said to learn from experience E wrt some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "\n",
    "- In the various problem settings T, P, and E can refer to completely different things. Some of the most popular tasks T in machine learning are the following:\n",
    "     \n",
    "  - Classification of an instance to one of the categories based on its features\n",
    "  - Regression - prediction of a numerical target feature based on other features of an instance\n",
    "  - Clustering - identifying partitions of instances based on the features of these instances so that the members within the groups are more similar to each other than those in the other groups\n",
    "  - Anomaly detection - search for instances that are \"greatly dissimilar\" to the rest of the sample or to some group of instances\n",
    "  - Ranking is a type of ML problem which is a mixture of Classification and Regression\n",
    "  \n",
    "  \n",
    "  \n",
    "- Experience, E refers to data. Machine learning algorithms can be divided into those that are trained in supervised or unsupervised manner. In unsupervised learning tasks, one has a set consisting of instances described by a set of features. In supervised learning problems, there's also a target variable, which is what we would like to be able to predict, known for each instance in a training set.\n",
    "\n",
    "- #### Example\n",
    "\n",
    "Classification and regression are supervised learning problems. For example, as a credit institution, we may want to predict loan defaults based on the data accumulated about our clients. Here, \n",
    "- experience E is the  available training data\n",
    "- a set of instances(clients)\n",
    "- a collection of features(such as age, salary, type of loan, past loan defaults, etc.) for each instance\n",
    "- target variable(whether they defaulted on the loan)\n",
    "\n",
    "This target variable is just a fact of loan default(1 or 0), hence a (binary) classification problem. If you were instead predicting by how much time the loan payment is overdue, this would become a regression problem.\n",
    "\n",
    "\n",
    "Finally, the third term used in the definition of machine learning is a metric of the algorithm's performance evaluation P. Such metrics differ for various problems and algorithms, and we will discuss them as we study new algorithms. For now, we will refer to a simple metric for classification algorithms, the proportion of correct answers - accuracy - on the test set.\n",
    "\n",
    "- #### In a nutshell,\n",
    "\n",
    "\n",
    "  - Context of Supervised Learning - Setting where we have data and labels\n",
    "  - Dataset consisting of rows (aka _instances_ or _observations) and columns (aka _features_)\n",
    "  - We are interested in predicting y (target feature)\n",
    "  - If the target feature contains values 0 and 1 => It's a _Binary Classification_ Problem\n",
    "  - If the target feature contains values from 0 to 9 (example - digit classification) => It's a _Multi-label Classification_ Problem \n",
    "  - If the target feature has numeric values => It's a _Regression_ Problem\n",
    "  \n",
    "\n",
    "\n",
    "Let's take a look at two supervised learning problems: classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "- For more info about writing equations in Jupyter Notebook refer [here](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree\n",
    "\n",
    "We begin our overview of classification and regression methods with one of the most popular ones - a decision tree. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. For example, the Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing paper on the institution portal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/dt1.png\" width = \"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of ML, one can see it as a simple classfier that determines the appropriate form of publication(book, article, chapter of the book, preprint, publication in the \"Higher School of Economics and the Media\") based on the content(book, pamphlet, paper), type of journal, original publication type(scientific journal, proceedings), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is often a generalization of the experts' experience, a means of sharing knowledge of a particular process. For example, before the introduction of scalable machine learning algorithms, the credit scoring task in the banking sector was solved by experts. The decision to grant a loan was made on the basis of some intuitively(or empirically) derived rules that could be represented as a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/dt2.png\" width = \"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our next case, we solve a binary classification problem (approve/deny a loan) on the grounds of \"Age\", \"Home-ownership\", \"Income\" and \"Education\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form \"feature _a_ value is less than _x_ and feature _b_ value is less than _y_ => Category 1\" into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: say, the client doesn't own a house and her income is less than 5000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see later, many other models, although more accurate, do not have this property and can be regarded as more of a \"black box\" approach, where it is harder to interpret how the input data was transformed into the output. Due to this understandability and similarity to human decision-making, decision trees have gained immense popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to build a Decision Tree?\n",
    "\n",
    "Earlier, we saw that the decision to grant a loan is made based on age, assets, income, and other variables. But what variable to look at first? Let's discuss a simple example where all the variables are binary.\n",
    "\n",
    "Recall the game of \"20 Questions\", which is often referenced when introducing decision trees. You've probably played this game -- one person thinks of a celebrity while the other tries to guess by asking only \"Yes\" or \"No\" questions. What question will the guesser ask first? Of course, they will ask the one that narrows down the number of the remaining options the most. Asking \"Is it Angelina Jolie?\" would, in the case of a negative response, leave all but one celebrity in the realm of possibility. In contrast, asking \"Is the celebrity a woman?\" would reduce the possibilities to roughly half. That is to say, the \"gender\" feature separates the celebrity dataset much better than other features like \"Angelina Jolie\", \"Spanish\", or \"loves football.\" This reasoning corresponds to the concept of information gain based on entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "\n",
    "We will build decision tree based on Shannon's entropy. This is a very important concept used in physics, information theory, and other areas.\n",
    "\n",
    "Shannon's entropy is defined for a system with _N_ possible states as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "S = - \\sum_{i=1}^N p_i log_2 p_i\n",
    "\\end{equation*}\n",
    "\n",
    "where $p_i$ is the probability of finding the system in the $i$-th state\n",
    "\n",
    "\n",
    "- Entropy can be described as a degree of chaos in the system\n",
    "- The higher the entropy, the less ordered the system and vice-versa\n",
    "- This will help us formalize \"effective data splitting\" which we alluded to in the context of \"20 Questions\"\n",
    "- Entropy is **non-negative** and can be anything upto infinity (Since probabilities are < 1, log value will be negative) \n",
    "- In case of binary classification, Entropy(S) lies b/w 0 and 1 with maximum value being 1\n",
    "\n",
    "\n",
    "_Understanding what 'chaos' mean in simpler context_\n",
    "\n",
    ">- Let's say we have a system of 5 Black balls and we need to find probability of picking up a black ball,<br>\n",
    "  >  P (probability) = 1 , S(entropy) = 0 (Since, P = 1, log(base2) 1 = 0 and hence S = 0) <br>\n",
    "  >  System is free of chaos\n",
    "  \n",
    ">- We have a system of 3 Black balls, 4 Red balls and 3 Green balls, we need to find probability of picking up a black ball,<br>\n",
    "  >  Here, P (probability) = 3/10 , S(entropy) = some value <br>\n",
    "  >  System will have chaos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Example\n",
    "\n",
    "To illustrate how entropy can help us identify good features for building a decision tree, let's look at a toy example. We will predict the color of the ball based on its position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/dt3.png\" width = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 blue and 11 yellow balls. If we randomly pull out a ball, then it will be blue with probability $p_1=\\frac{9}{20}$ and yellow with probability $p_2 =\\frac{11}{20}$, which gives us an entropy $S_0 = -\\frac{9}{20}$$log_2\\frac{9}{20} - \\frac{11}{20}log_2\\frac{11}{20} \\approx 1$. <br>\n",
    "\n",
    "This value by itself may not tell us much, but let's see how the value changes if we were to break the balls into two groups: with the position less than or equal to 12 and greater than 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/dt4.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left group has 13 balls, 8 blue and 5 yellow. The entropy of this group is $S_1 = -\\frac{5}{13}log_2\\frac{5}{13} - \\frac{8}{13}log_2\\frac{8}{13} \\approx 0.96$. <br>\n",
    "The right group has 7 balls, 1 blue and 6 yellow. The entropy of this group is $S_2 = -\\frac{1}{7}log_2\\frac{1}{7} - \\frac{6}{7}log_2\\frac{6}{7} \\approx 0.6$. <br>\n",
    "As you can see, entropy has decreased in both groups, more so in the right group. Since entropy is, in fact, the degree of chaos(or uncertainty) in the system, the reduction in entropy is called **information gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typically the start of the problem in a Decision Tree is called **Root**.\n",
    "- Then there are Intermediate Nodes and End Nodes\n",
    "- End nodes, called **Leaves** are different from intermediate nodes in the sense that they don't have children\n",
    "- The whole setup is called **Classification Tree**\n",
    "- Based on whether the conditions are satisfied or not, the sub-conditions are branched out\n",
    "- But the rules/conditions need to be generated automatically\n",
    "- Main idea is to have a Training set and then *generate the Decision Tree automatically*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Build a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "#### Information Gain(IG)\n",
    "\n",
    "- Since entropy is, in fact, the degree of chaos(or uncertainty) in the system, the reduction in entropy is called information gain\n",
    "- **IG** for a split based on the variable Q(example, X < = 12) is defined as \n",
    "  - 'Split'  is nothing but the branches in the tree model\n",
    "  - 'Q' is the condition on which the split is done\n",
    "\n",
    "\\begin{equation*}\n",
    "IG(Q) = S_0 - \\sum_{i=1}^q \\frac{N_i}{N} S_i \n",
    "\\end{equation*}\n",
    "\n",
    "- q : Number of groups after the split\n",
    "- Ni : Number of objects from the sample i.e. events present in one side of the branch or split\n",
    "- N : Total events i.e. sample size\n",
    "- S0 : Initial Entropy\n",
    "- Si : The entropies of each branch in a split\n",
    "\n",
    "Example:\n",
    "\n",
    "Let the split yields two groups (_q=2_), one with 13 elements(_N1 = 13_), the other with 7(_N2 = 7_). Therefore, we can compute the information gain as \n",
    "\n",
    "\\begin{equation*}\n",
    "IG(x\\leq12) = S_0 - \\frac{13}{20}S_1 - \\frac{7}{20}S_2 \\approx 0.16 \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlprojects",
   "language": "python",
   "name": "myenv"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
